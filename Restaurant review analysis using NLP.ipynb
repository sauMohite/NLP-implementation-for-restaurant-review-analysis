{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HcWTxzQ2Laer"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Import dataset\n",
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# library to clean data\n",
        "import re\n",
        "\n",
        "# Natural Language Tool Kit\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# to remove stopword\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for Stemming propose\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Initialize empty array\n",
        "# to append clean text\n",
        "corpus = []\n",
        "\n",
        "# 1000 (reviews) rows to clean\n",
        "for i in range(0, 1000):\n",
        "\t\n",
        "\t# column : \"Review\", row ith\n",
        "\treview = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "\t\n",
        "\t# convert all cases to lower cases\n",
        "\treview = review.lower()\n",
        "\t\n",
        "\t# split to array(default delimiter is \" \")\n",
        "\treview = review.split()\n",
        "\t\n",
        "\t# creating PorterStemmer object to\n",
        "\t# take main stem of each word\n",
        "\tps = PorterStemmer()\n",
        "\t\n",
        "\t# loop for stemming each word\n",
        "\t# in string array at ith row\n",
        "\treview = [ps.stem(word) for word in review\n",
        "\t\t\t\tif not word in set(stopwords.words('english'))]\n",
        "\t\t\t\t\n",
        "\t# rejoin all string array elements\n",
        "\t# to create back into a string\n",
        "\treview = ' '.join(review)\n",
        "\t\n",
        "\t# append each string to create\n",
        "\t# array of clean text\n",
        "\tcorpus.append(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvSWYw6wLrLo",
        "outputId": "72345509-7ba5-4922-b655-11b6534bc139"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# To extract max 1500 feature.\n",
        "# \"max_features\" is attribute to\n",
        "# experiment with to get better results\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "\n",
        "# X contains corpus (dependent variable)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "# y contains answers if review\n",
        "# is positive or negative\n",
        "y = dataset.iloc[:, 1].values\n"
      ],
      "metadata": {
        "id": "jgOLJ8NLLxe4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into\n",
        "# the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# experiment with \"test_size\"\n",
        "# to get better results\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "metadata": {
        "id": "ViiGkngUL-C-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Random Forest Classification\n",
        "# to the Training set\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# n_estimators can be said as number of\n",
        "# trees, experiment with n_estimators\n",
        "# to get better results\n",
        "model = RandomForestClassifier(n_estimators = 501,\n",
        "\t\t\t\t\t\t\tcriterion = 'entropy')\n",
        "\t\t\t\t\t\t\t\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFhOVqwPMBsH",
        "outputId": "28372a9a-e8e2-4e9b-bdb0-5a9cc859e137"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=501)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP_oTn1ZM0-1",
        "outputId": "9b44ee9c-ea70-442d-c335-10fb5dc595ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
              "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK8WV5e6M5vF",
        "outputId": "9ba8e9e6-4e25-449c-e308-90b77694698f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[111,  13],\n",
              "       [ 42,  84]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}